{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_unlab = pd.read_csv('AllBooks_baseline_DTM_Unlabelled.csv')\n",
    "religion_lab = pd.read_csv('AllBooks_baseline_DTM_Labelled.csv')\n",
    "\n",
    "print(religion_lab.shape)\n",
    "religion_lab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_unlab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_lab['label'] = religion_lab['Unnamed: 0'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "#for i in range(len(religion_lab)):\n",
    "#    religion_lab['label'][i] = religion_lab['Unnamed: 0'][i].split('_')[0]\n",
    "   \n",
    "\n",
    "labels = religion_lab['label'].value_counts()\n",
    "labels = pd.DataFrame(labels).reset_index()\n",
    "labels.columns = ['label', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (9, 6))\n",
    "sns.barplot(data = labels, y = 'label', x = 'count', color = 'dodgerblue')\n",
    "fig.suptitle('Żródła tekstów', fontsize=18)\n",
    "plt.xlabel('Liczba tesktów')\n",
    "plt.ylabel(\"Księga\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_pop_words = religion_unlab.sum().sort_values(ascending=False).head(30)\n",
    "most_pop_words = pd.DataFrame(most_pop_words).reset_index()\n",
    "most_pop_words.columns = ['word', 'count']\n",
    "\n",
    "fig = plt.figure(figsize = (9, 6))\n",
    "sns.barplot(data = most_pop_words, x = 'word', y = 'count', color = 'dodgerblue')\n",
    "plt.xticks(rotation=60)\n",
    "fig.suptitle('Najpopularniejsze słowa', fontsize=18)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Liczba wystąpień we wszystkich tekstach')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=100,\n",
    "        max_font_size=30,\n",
    "        scale=3,\n",
    "        random_state=1)\n",
    "   \n",
    "    wordcloud=wordcloud.generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(religion_unlab.sum().sort_values(ascending=False).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_lab_stacked = religion_lab.drop('Unnamed: 0', axis = 1).groupby('label').sum().transpose()\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, figsize = (15, 24))\n",
    "i = 0\n",
    "\n",
    "for c in religion_lab_stacked.columns:\n",
    "    df = pd.DataFrame(religion_lab_stacked[c].sort_values(ascending = False)).reset_index().head(10)\n",
    "    df.columns = ['word', 'count']\n",
    "    sns.barplot(data = df, x = 'word', y = 'count', color = 'dodgerblue', ax = ax[int(np.floor(i/2)%4), int(i%2)])\n",
    "    ax[int(np.floor(i/2)%4), int(i%2)].set_title(c)\n",
    "    ax[int(np.floor(i/2)%4), int(i%2)].set_xlabel('')\n",
    "    ax[int(np.floor(i/2)%4), int(i%2)].set_xticklabels(ax[int(np.floor(i/2)%4), int(i%2)].get_xticklabels(), rotation=45)\n",
    "\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textstat import lexicon_count \n",
    "from textstat import flesch_reading_ease \n",
    "from textstat import flesch_kincaid_grade \n",
    "from textstat import sentence_count \n",
    "from textstat import lexicon_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Complete_data .txt', 'r')\n",
    "file_content = file.read()\n",
    "file.close()\n",
    "\n",
    "content_list = re.split('\\d+\\.\\d+', file_content)\n",
    "\n",
    "text = []\n",
    "for i in range(len(content_list)):\n",
    "    stripped = content_list[i].strip()\n",
    "    if stripped != '':\n",
    "        text.append(content_list[i])\n",
    "        \n",
    "substrings_to_drop = ['\\n', ' \\n', '\\n ', '  \\n', '§', '§ ']\n",
    "            \n",
    "for i in range(len(corpus)):\n",
    "    for j in substrings_to_drop:\n",
    "        corpus[i] = corpus[i].replace(j, '') \n",
    "        \n",
    "data = pd.DataFrame(corpus).reset_index()\n",
    "data.columns = ['index', 'text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#characters\n",
    "data['len'] = data['text'].str.len()\n",
    "#words\n",
    "data['words'] = data['text'].apply(lambda x : lexicon_count(x, removepunct=True))\n",
    "#average sentence length\n",
    "data['avg_sen'] = data['text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n",
    "#reading ease\n",
    "data['reading_ease'] = data['text'].apply(lambda x : flesch_reading_ease(x))\n",
    "#flesch_kincaid_grade\n",
    "data['grade'] = data['text'].apply(lambda x : flesch_kincaid_grade(x))\n",
    "#sentences\n",
    "data['sentences'] = data['text'].apply(lambda x : sentence_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 5))\n",
    "sns.histplot(data = data, x = 'len', ax = ax1)\n",
    "sns.histplot(data['words'], ax = ax2)\n",
    "ax1.set_title('Rozkład liczby znaków w tekście')\n",
    "ax2.set_title('Rozkład liczby słów w tekście')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 5))\n",
    "sns.histplot(np.log1p(data['sentences']), ax = ax1)\n",
    "sns.histplot(data = data, x = 'avg_sen', ax = ax2)\n",
    "ax1.set_title('Rozkład liczby zdać w tekstach (skala logarytmiczna)')\n",
    "ax2.set_title('Rozkład średniej iczby zdań w tekstach')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 5))\n",
    "sns.histplot(data = data, x = 'reading_ease', ax = ax1)\n",
    "sns.histplot(data['grade'], ax = ax2)\n",
    "ax1.set_title('Flesh reading ease')\n",
    "ax2.set_title('Flesh-Kincaid grade level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-editor",
   "metadata": {},
   "source": [
    "## Druga część"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AllBooks_baseline_DTM_Unlabelled.csv\")\n",
    "print(f\"Shape of data: {df.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-johnston",
   "metadata": {},
   "source": [
    "Mamy 8266 słów (kolumn), 590 rekordów. Wszystkie wartości są dodatnimi wartościami. Nie mamy braków w danych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-administrator",
   "metadata": {},
   "source": [
    "Na początek sprawdźmy czy mamy w naszej ramce danych tzw. skrótowce, czyli słówka typu \"don't\",\"aren't\", isn't\" itp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {\"# foolishness\":\"foolishness\"}, inplace = True)\n",
    "for i in df.columns:\n",
    "    if \"'\" in i: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-binding",
   "metadata": {},
   "source": [
    "Wniosek: nie mamy skrótowców, więc możemy pominąć punkt ich rozwijania. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-seminar",
   "metadata": {},
   "source": [
    "Z naszych słów wyciągnijmy korzeń. Może się zdarzyć, że mamy jednocześnie 2 różne formy tego samego wyrazu, np. 'play', 'playing', 'plays'. Dla naszego zadania jest to oczywiście jedno i to samo słowo. Zrobimy to ponownie wykorzystując bibliotekę Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "listToStr = ' '.join([str(elem) for elem in df.columns])\n",
    "doc = nlp(listToStr)\n",
    "\n",
    "i=0\n",
    "tokenDict = {}\n",
    "for token in doc:\n",
    "    if (str(token) != str(token.lemma_)): \n",
    "        tokenDict[str(token)] = token.lemma_\n",
    "        \n",
    "print(tokenDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = tokenDict, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kolumny unikalne: {len(df.columns.unique())}.\")\n",
    "print(f\"Wszystkie kolumny: {len(df.columns)}.\")\n",
    "print(\"Przyklad recznie znaleziony zduplikowanych kolumn\")\n",
    "df[\"oppose\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-breeding",
   "metadata": {},
   "source": [
    "Pozbadzmy sie duplikujacych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sum(axis=1, level=0)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-pocket",
   "metadata": {},
   "source": [
    "Sprawdźmy teraz czy mamy słówka zaliczane do grupy 'najpopularniejszych słówek języka'. W języku angielskim są to słówka typu “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc. Ponownie wykorzystamy bibliotekę spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "nlp = English()\n",
    "stopwords = []\n",
    "for i in df.columns:\n",
    "    lexeme = nlp.vocab[i]\n",
    "    if lexeme.is_stop == True: stopwords.append(i)\n",
    "print(stopwords)\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(stopwords)} słów z naszej ramki zostało zklasyfikowane jako słowa o niskiej wartości dla całościowego znaczenia tekstu. Spośród ponad 6000 wszystkich słów, stanowią one niewielki procent więc możemy je usunąć.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = stopwords)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-morgan",
   "metadata": {},
   "source": [
    "Teraz sprawdźmy najczęściej pojawiające się słówka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_pop_words = df.sum().sort_values(ascending=False).head(30)\n",
    "most_pop_words = pd.DataFrame(most_pop_words).reset_index()\n",
    "most_pop_words.columns = ['word', 'count']\n",
    "\n",
    "fig = plt.figure(figsize = (9, 6))\n",
    "sns.barplot(data = most_pop_words, x = 'word', y = 'count', color = 'dodgerblue')\n",
    "plt.xticks(rotation=60)\n",
    "fig.suptitle('Most popular words in whole dataset', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Średnia liczba wystąpień jednego słowa: {np.mean(most_pop_words['count']).round(2)}\")\n",
    "print(f\"Odchylenie standardowe liczby wystąpień jednego słowa: {np.std(most_pop_words['count']).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'word': df.columns}\n",
    "word_len = pd.DataFrame(data = d)\n",
    "word_len['nchars'] = word_len['word'].apply(lambda x: len(x))\n",
    "word_len['occurences'] = word_len['word'].apply(lambda x: df[x].sum())\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (9, 6))\n",
    "word_len['nchars'].plot(kind = 'hist', title = 'Rozkład długości wyrazów ze wszystkich tekstów', bins = 25\n",
    "                           , xlabel = \"Liczba liter\", ylabel = 'Liczba słów o danej długości')\n",
    "plt.show()\n",
    "print(f\"Średnia długość słowa: {np.mean(word_len['nchars']).round(2)}\")\n",
    "print(f\"Odchylenie standardowe: {np.std(word_len['nchars']).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdźmy te słowa, które sa bardzo długie albo krótkie\n",
    "short_words = word_len.loc[word_len['nchars'] == 2]\n",
    "short_words = short_words['word'].to_numpy()\n",
    "\n",
    "long_words = word_len.loc[word_len['nchars'] >= 17]\n",
    "long_words = long_words['word'].to_numpy()\n",
    "\n",
    "print(word_len.loc[word_len['nchars'] == 2])\n",
    "print(word_len.loc[word_len['nchars'] >= 17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponieważ liczby wystąpień tych słów są dużo niższe niż odchylenie standardowe, również usuniemy je z ramki danych\n",
    "\n",
    "df = df.drop(columns = (short_words), axis = 1)\n",
    "df = df.drop(columns = (long_words), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-touch",
   "metadata": {},
   "source": [
    "Oceńmy teraz czy nasze słowa są nacechowane pozytywnie czy negatywnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def sentiment(x):\n",
    "    if x<0:\n",
    "        return 'neg'\n",
    "    elif x==0:\n",
    "        return 'neu'\n",
    "    else:\n",
    "        return 'pos'\n",
    "    \n",
    "def subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "    \n",
    "word_len['polarity_score']=word_len['word'].\\\n",
    "   apply(lambda x : polarity(x))\n",
    "\n",
    "word_len['polarity']=word_len['polarity_score'].\\\n",
    "   map(lambda x: sentiment(x))\n",
    "\n",
    "word_len['subjectivity']=word_len['word'].\\\n",
    "   map(lambda x: subjectivity(x))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (15, 6))\n",
    "print(\"Nacechowanie emocjonalne słów:\")\n",
    "word_len['polarity_score'].hist(ax = axs[0])\n",
    "word_len['polarity'].hist(ax = axs[1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot_sub = word_len['subjectivity'].hist()\n",
    "plot_sub.set_title(\"Obiektywność\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-breach",
   "metadata": {},
   "source": [
    "Wniosek: większość słów z naszej bazy ma neutralne nacechowanie emocjonalne. Z pozostałych nielicznych słów, większość jest nacechowana pozytywnie. Słowa są również raczej obiektywne."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
