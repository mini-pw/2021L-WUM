{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-circumstances",
   "metadata": {},
   "source": [
    "# Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ramka danych ze słowawmi\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-momentum",
   "metadata": {},
   "source": [
    "## Skalowanie ramki za pomocą TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "texts = [''] * len(df)\n",
    "for i in range(len(df)):\n",
    "    t = texts[i]\n",
    "    tmp_num = np.array(df.iloc[i])\n",
    "    for j in range(len(tmp_num)):\n",
    "        w = int(tmp_num[j])\n",
    "        for k in range(w): t = t + ' ' + cols[j]\n",
    "    texts[i] = str(t)\n",
    "    #print(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=2, use_idf=True, stop_words='english', token_pattern=r\"\\b[^\\d\\W]+\\b\")\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf.toarray(), columns=list(tfidf_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-massachusetts",
   "metadata": {},
   "source": [
    "## Wczytanie i standaryzacja statystyk tekstów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ramka danych ze statystykami tesktów\n",
    "stats = pd.read_csv('stats_df.csv')\n",
    "stats = stats.drop(['Unnamed: 0', 'index', 'text'], axis = 1)\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(stats)\n",
    "stat_scale = scaler.transform(stats)\n",
    "\n",
    "stats_scale = pd.DataFrame(stat_scale, columns = stats.columns)\n",
    "stats_scale.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-lightweight",
   "metadata": {},
   "source": [
    "## Stworzenie zbioru do klasteryzacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(stats_scale.reset_index(), df_tfidf.reset_index(), on = 'index').drop('index', axis = 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-spencer",
   "metadata": {},
   "source": [
    "## Stworzenie ramki z odpowiedziami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_csv('AllBooks_baseline_DTM_Labelled.csv')[['Unnamed: 0']]\n",
    "Y['label'] = Y['Unnamed: 0'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "\n",
    "def add_religion(label):\n",
    "  if label == \"Buddhism\": return \"Buddhism\"\n",
    "  elif label == \"TaoTeChing\": return \"Taoism\"\n",
    "  elif (label == \"Upanishad\") | (label ==\"YogaSutra\"): return \"Hindusim\"\n",
    "  else: return \"Old testament\"\n",
    "\n",
    "    \n",
    "Y['rel'] = Y['label'].apply(lambda x : add_religion(x))\n",
    "Y = Y.drop('Unnamed: 0', axis = 1)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-hacker",
   "metadata": {},
   "source": [
    "# Klasteryzacja bez redukcji wymiarów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-update",
   "metadata": {},
   "source": [
    "## Wyznaczenie liczby klastrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metdoda łokcia dla KMeans\n",
    "\n",
    "def KMeansElbow(X, k_max):\n",
    "    #  WCSS = within-cluster sum of squares\n",
    "    scores = []\n",
    "    for k in range(1, k_max+1):\n",
    "        model = KMeans(n_clusters=k, random_state=0)\n",
    "        model.fit(X)\n",
    "        wcss = model.score(X) * -1 # score returns -WCSS\n",
    "        scores.append(wcss)\n",
    "    x_ticks = list(range(1, len(scores) + 1))\n",
    "    plt.plot(x_ticks, scores, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Within-cluster sum of squares')\n",
    "    plt.title('The Elbow Method showing the optimal k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeansElbow(X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, rand_score, adjusted_mutual_info_score, mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metoda silhouette\n",
    "\n",
    "def silhouetteClusterNum(X, cluster_num, score_fun):        \n",
    "    scores = []    \n",
    "    for k in range(2, cluster_num+1):\n",
    "        model_instance = KMeans(n_clusters=k, random_state=0)\n",
    "        labels = model_instance.fit_predict(X)\n",
    "        wcss = score_fun(X, labels)\n",
    "        scores.append(wcss)\n",
    "    \n",
    "    f = plt.figure(figsize=[8, 6])\n",
    "    plt.plot(range(2, cluster_num+1), scores, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel(f'{score_fun}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouetteClusterNum(X, 10, silhouette_score)\n",
    "#im większy wynik tym lepiej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouetteClusterNum(X, 10, davies_bouldin_score)\n",
    "#im mniejszy wybik tym lepiej"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-universal",
   "metadata": {},
   "source": [
    "Biorąc pod uwagę wyniki różnych metryk sprawdzimy podział na 2, 3, 4 i 5 klastrów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-mother",
   "metadata": {},
   "source": [
    "## Stworzenie ramek z redukcją wymiarów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-anthony",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_),marker='x')\n",
    "plt.xlabel('number of components')\n",
    "plt.xlim(0, 40)\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-belle",
   "metadata": {},
   "source": [
    "dla 3 zmiennych mamy 85% wariancji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca2 = PCA(n_components=2).fit_transform(X)\n",
    "X_pca2 = pd.DataFrame({'x': X_pca2[:, 0], 'y': X_pca2[:, 1], 'label': Y['label'], 'rel': Y['rel']})\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=[18, 6])\n",
    "sns.scatterplot(data=X_pca2, x='x', y='y', hue='label', ax = ax1)\n",
    "sns.scatterplot(data=X_pca2, x='x', y='y', hue='rel', ax = ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_pca3 = PCA(n_components=3).fit_transform(X)\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "Scene = dict(xaxis = dict(title  = 'PC1'),yaxis = dict(title  = 'PC2'),zaxis = dict(title  = 'PC3'))\n",
    "labels = le.fit_transform(Y['label'])\n",
    "trace = go.Scatter3d(x=X_pca3[:,0], y=X_pca3[:,1], z=X_pca3[:,2], mode='markers',marker=dict(color = labels, size = 10, line = dict(color = 'gray',width = 5)))\n",
    "layout = go.Layout(margin=dict(l=0,r=0),scene = Scene, height = 600,width = 600)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scene = dict(xaxis = dict(title  = 'PC1'),yaxis = dict(title  = 'PC2'),zaxis = dict(title  = 'PC3'))\n",
    "labels = le.fit_transform(Y['rel'])\n",
    "trace = go.Scatter3d(x=X_pca3[:,0], y=X_pca3[:,1], z=X_pca3[:,2], mode='markers',marker=dict(color = labels, size = 10, line = dict(color = 'gray',width = 5)))\n",
    "layout = go.Layout(margin=dict(l=0,r=0),scene = Scene, height = 600,width = 600)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-finnish",
   "metadata": {},
   "source": [
    "## Klasteryzacja bez redukcji wymiarów, ale zwizualizowana na PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeansClustering(data, reduction, actual_labels):\n",
    "    results = pd.DataFrame(columns = ['clusters', 'silhouette_score', 'davies_bouldin_score',\n",
    "                                      'rand_score', 'adjusted_mutual_info_score', 'mutual_info_score'])\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 4, figsize = (18, 5))\n",
    "\n",
    "    for i in range(2, 6):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=0)\n",
    "        kmeans.fit(data)\n",
    "        y_kmeans = kmeans.predict(data)\n",
    "        \n",
    "        i_results = pd.DataFrame({'clusters':[i],\n",
    "                                  'silhouette_score':[silhouette_score(data, y_kmeans)],\n",
    "                                  'davies_bouldin_score':[davies_bouldin_score(data, y_kmeans)],\n",
    "                                  'rand_score':[rand_score(actual_labels, y_kmeans)],\n",
    "                                  'adjusted_mutual_info_score':[adjusted_mutual_info_score(actual_labels, y_kmeans)],\n",
    "                                  'mutual_info_score':[mutual_info_score(actual_labels, y_kmeans)]})\n",
    "        results = pd.concat([results, i_results])\n",
    "\n",
    "        sns.scatterplot(data = reduction, x = 'x', y = 'y',\n",
    "                        hue = y_kmeans, legend = False,\n",
    "                        ax = axs[i-2])\n",
    "        ax1.set_title(f'{i} clusters')\n",
    "        \n",
    "    \n",
    "    plt.show()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeansClustering(X, X_pca2, Y['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AggClustering(data, reduction, actual_labels):\n",
    "    results = pd.DataFrame(columns = ['clusters', 'linkage', 'silhouette_score', 'davies_bouldin_score',\n",
    "                                     'rand_score', 'adjusted_mutual_info_score', 'mutual_info_score'])\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 4, figsize = (18, 15))\n",
    "    linkage = ['ward', 'complete', 'single']\n",
    "\n",
    "    for j in range(3):\n",
    "        for i in range(2, 6):\n",
    "            aggClus = AgglomerativeClustering(n_clusters = i, linkage = linkage[j])\n",
    "            y_aggClus = aggClus.fit_predict(data)\n",
    "            \n",
    "            i_results = pd.DataFrame({'clusters':[i],\n",
    "                                  'linkage':[linkage[j]],    \n",
    "                                  'silhouette_score':[silhouette_score(data, y_aggClus)],\n",
    "                                  'davies_bouldin_score':[davies_bouldin_score(data, y_aggClus)],\n",
    "                                  'rand_score':[rand_score(actual_labels, y_aggClus)],\n",
    "                                  'adjusted_mutual_info_score':[adjusted_mutual_info_score(actual_labels, y_aggClus)],\n",
    "                                  'mutual_info_score':[mutual_info_score(actual_labels, y_aggClus)]})\n",
    "            results = pd.concat([results, i_results])\n",
    "\n",
    "            sns.scatterplot(data = reduction, x = 'x', y = 'y',\n",
    "                            hue = y_aggClus, legend = False,\n",
    "                            ax = axs[j, i-2])\n",
    "            axs[j, i-2].set_title(f'{i} clusters, {linkage[j]} linkage')\n",
    "\n",
    "    plt.show()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "AggClustering(X, X_pca2, Y['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-bradley",
   "metadata": {},
   "source": [
    "## Klastrowanie po PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeansElbow(X_pca2[['x','y']], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouetteClusterNum(X_pca2[['x','y']], 10, silhouette_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouetteClusterNum(X_pca2[['x','y']], 10, davies_bouldin_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-translator",
   "metadata": {},
   "source": [
    "Po PCA dalej wygląda na to, że będziemy szukac tej samej liczby klastrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeansClustering(X_pca2[['x', 'y']], X_pca2, Y['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "AggClustering(X_pca2[['x', 'y']], X_pca2, Y['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-thirty",
   "metadata": {},
   "source": [
    "# T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.figure(figsize=[10, 8])\n",
    "tSNE = TSNE(random_state=0, verbose=1)\n",
    "X_tsne = tSNE.fit_transform(X)\n",
    "X_tsne = pd.DataFrame({'x': X_tsne[:, 0], 'y': X_tsne[:, 1], 'label': Y['label'], 'rel' : Y['rel']})\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=[18, 6])\n",
    "sns.scatterplot(data=X_tsne, x='x', y='y', hue='label', ax = ax1)\n",
    "sns.scatterplot(data=X_tsne, x='x', y='y', hue='rel', ax = ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-accused",
   "metadata": {},
   "source": [
    "## Klasteryzacja bez redukcji wymiarów, wizualizacja na T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeansClustering(X, X_tsne, Y['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "AggClustering(X, X_tsne, Y['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-organic",
   "metadata": {},
   "source": [
    "## Klasteryzacja po T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeansClustering(X_tsne[['x', 'y']], X_tsne, Y['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "AggClustering(X_tsne[['x', 'y']], X_tsne, Y['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-airfare",
   "metadata": {},
   "source": [
    "# Wyniki w porównaniu do religii a nie konkretnych ksiąg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bez redukcji wymiarów\n",
    "KMeansClustering(X, X_tsne, Y['rel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bez redukcji wymiarów\n",
    "AggClustering(X, X_tsne, Y['rel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#po PCA\n",
    "KMeansClustering(X_pca[['x', 'y']], X_tsne, Y['rel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#po PCA\n",
    "AggClustering(X_pca[['x', 'y']], X_tsne, Y['rel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#po t-sne\n",
    "KMeansClustering(X_tsne[['x', 'y']], X_tsne, Y['rel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#po t-sne\n",
    "AggClustering(X_tsne[['x', 'y']], X_tsne, Y['rel'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
