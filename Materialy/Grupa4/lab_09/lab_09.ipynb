{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "lab_09.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ifx29Ud5Vis"
      },
      "source": [
        "# Wstp do sztucznych sieci neuronowych\n",
        "### Czym s sieci neuronowe?\n",
        "Najprociej -- uniwersalnym aproksymatorem funkcji. To, co jest wa偶ne, to 偶e bdziemy uczy sie neuronow aproksymowa pewn (wybran przez nas) funkcj.\n",
        "\n",
        "\n",
        "### A w praktyce?\n",
        "W praktyce sieci neuronowe prezentowane s jako zo偶enie kombinacji liniowych i nieliniowych przeksztace. Brzmi skomplikowanie, dlatego zacznijmy bardzo prostego przykadu, sieci z jednym neuronem:\n",
        "\n",
        "\n",
        "![](https://www.researchgate.net/publication/286020106/figure/fig13/AS:328309167673363@1455286414002/Basic-artificial-neural-network-cell-artificial-neuron.png)\n",
        "殴r贸do: H眉seyin Ceylan, An Artificial Neural Networks Approach to Estimate\n",
        "Occupational Accident: A National Perspective for Turkey, 2014\n",
        "\n",
        "W prakyce sieci neuronowe wizualizowane s przez tego typu obrazki:\n",
        "![](https://visualstudiomagazine.com/articles/2014/11/01/~/media/ECG/visualstudiomagazine/Images/2014/11/1114vsm_mccaffreyfig2.ashx)\n",
        "殴r贸do: https://visualstudiomagazine.com/articles/2014/11/01/use-python-with-your-neural-networks.aspx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA80uCnw5Viu"
      },
      "source": [
        "Sp贸jrzmy teraz na sie w nieco inny spos贸b ni偶 na powy偶szych obrazkach. Pomylmy, jak opisa j wzorem. Z pominiciem skadowej staej mo偶emy powy偶sz sie opisa wzorem:\n",
        "\n",
        "$$ y = W_2 \\cdot f(W_1 \\cdot x) $$\n",
        "\n",
        "Gdzie:  \n",
        "$ y $  -- wektor wyj  \n",
        "$ x $  -- wektor wej  \n",
        "$ f() $ -- nieliniowa funkcja $ \\mathbb{R}^n \\to \\mathbb{R}^n $  \n",
        "$ W_1 $, $ W_2 $ -- macierze wag odpowiednich warstw\n",
        "\n",
        "Dlaczego wa偶ny jest zapis macierzowy? Komputer bardzo dobrze zr贸wnolegla obliczenia macierzowe. W szczeg贸lnoci karty graficzne specjalizuj si w obliczeniach macierzowych. Dlatego trening sieci na karcie graficznej jest zazwyczaj kilkadziesit razy szybszy ni偶 na procesorze."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yQyuIrE5Viu"
      },
      "source": [
        "### Jak wytrenowa sie neuronow?\n",
        "Do tej pory m贸wilimy o tym, jak sieci neuronowe wyznaczaj wyjcie dla zadanego wejcia. Ale jak je nauczy? Odpowiedzi jest algorytm wstecznej propagacji gradientu. Na wysokim poziomie dziaa to tak:\n",
        "* Dajemy sieci wejcie i m贸wimy: zgadnij, jaka jest odpowied藕\n",
        "![](https://hmkcode.com/images/ai/bp_forward.png)\n",
        "* Sie zwraca nam wyjcie, a my liczymy ile si pomylia\n",
        "![](https://hmkcode.com/images/ai/bp_error.png)\n",
        "* Wyznaczamy r贸偶niczki bdu po poszczeg贸lnych wagach. Liczymy to w do sprytny spos贸b, ale nie bdziemy si w to teraz wgbia\n",
        "* Znajc t r贸偶niczki, mo偶emy powiedzie jak mocno poszczeg贸lne wagi wpyny na uzyskany bd. Dlatego aktualizujemy wagi wedug wzoru:\n",
        "$$ W_{k, i, j} = W_{k, i, j} - a \\cdot {{\\partial E}\\over{\\partial W_{k, i, j}}}$$\n",
        "Gdzie:  \n",
        "$W_{k, i, j}$ -- waga poczenia z $i$-tego neuronu $k-1$ warstwy do $j$-tego neuronu $k$-tej warstwy  \n",
        "$E$ -- obliczona \"r贸偶nica\" midzy wartoci zwr贸con a t, kt贸rej si spodziewalimy  \n",
        "$a$ -- staa uczenia\n",
        "\n",
        "W rzeczywistoci takie uczenie byoby bardzo niestabilne, poniewa偶 ka偶dy punkt ze zbioru uczcego \"cignby\" parametry w swoj stron. Dlatego uczymy w tak zwanych mini-batchach skadajcych si z kilkunastu -- kilkuset punkt贸w jednoczenie. Pochodne obliczone dla ka偶dego z punkt贸w z batcha dodajemy do siebie i otrzymujemy \"redni kierunek poprawiajcy\". W tej spos贸b sie uczy si znacznie bardziej stabilnie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WafhkozW5Viv"
      },
      "source": [
        "### A jak to zaprogramowa?\n",
        "* Mo偶na napisa wszystko rcznie -- zapewne czeka Was jeszcze taki projekt.\n",
        "* Mo偶na skorzysta z dedykowanych bibliotek, na przykad `pytorch`, `tensorflow` czy `keras` \n",
        "* Mo偶na skorzysta z `sklearn.neural_network`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SzKjSRE5Viw"
      },
      "source": [
        "### Przykad 1. -- klasyfikacja\n",
        "W zwizku z tym, 偶e sie neuronowa mo偶e mie wiele wyj, bardzo naturalne jest jej wykorzystanie do zadania klasyfikacji wieloklasowej. Pomys jest taki, 偶eby $i$-te wyjcie m贸wio o prawdopodobiestwie przynale偶noci do $i$-tej klasy. Aby to osign, normalizuje si wyjcia funkcj sofmax:\n",
        "$$ {{e^{y_i}}\\over {\\sum_{j=1}^K e^{y_j}}} $$\n",
        "\n",
        "Gdzie:  \n",
        "$y_i$ -- $i$-te wyjcie  \n",
        "$K$ -- liczba klas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IboE2mNN5Vix"
      },
      "source": [
        "Zbi贸r mnist zawiera rcznie pisane cyfry.\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "呕r贸do: https://en.wikipedia.org/wiki/MNIST_database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HZsL6-35Viy"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.activations import *\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye9X8onuErEF"
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
        "plt.imshow(X_train_full[0], cmap='binary')\n",
        "y_train_full.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH6c4IjUEsko"
      },
      "source": [
        "X_train, y_train = X_train_full[:50_000] / 255., y_train_full[:50_000]\n",
        "X_valid, y_valid = X_train_full[50_000:] / 255., y_train_full[50_000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZW75Aq_FTMX"
      },
      "source": [
        "Do stworzenia modelu wykorzystamy model sekwencyjny. W ten spos贸b dane bd przetwarzane po kolei przez wszystkie warstwy, kt贸re dodamy. \n",
        "\n",
        "Poza interfejsem sekwencyjnym w bibliotece Tensorflow wystpuje tak偶e interfejs funkcjonalny oraz podklasowy, ale o nich dzisiaj nie bdzie.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxnmE6LDE2Fg"
      },
      "source": [
        "def build_model(hidden_layer_size):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28,28)))\n",
        "    model.add(Dense(hidden_layer_size, activation='relu'))\n",
        "    model.add(Dense(hidden_layer_size, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-lkv1-RFEWR"
      },
      "source": [
        "Tensorflow wymaga kompilacji modelu. Przy okazji musimy okreli funkcj bdu, optymalizator oraz metryki su偶ce do oceny modelu. W naszym przypadku wykorzystamy optymalizator Stochastic Gradient Descent (SGD). Musimy tak偶e poda krok uczenia - learninig rate. **Jest to najwa偶niejszy hiperparametr przy uczeniu modeli gbokich.** Najwicej czasu powinno by powicone wanie na jego poprawne dobranie. W og贸lnoci nie musi on by stay, ale zmienia si np wraz z numerem iteracji fazy uczenia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDNAClapE2J8"
      },
      "source": [
        "model = build_model(256)\n",
        "lr = 2e-1\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "             optimizer=SGD(learning_rate=lr),\n",
        "             metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ha2PG34E9Vg"
      },
      "source": [
        "h = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3TINZYpL5p1"
      },
      "source": [
        "plt.plot(h.history['loss'], label='loss')\n",
        "plt.plot(h.history['val_loss'], label='val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEAJRMkOVqGk"
      },
      "source": [
        "plt.plot(h.history['accuracy'], label='accuracy')\n",
        "plt.plot(h.history['val_accuracy'], label='val_accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGsS9ftVTyML"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "conf_m = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "row_sums = conf_m.sum(axis=1)\n",
        "conf_m = conf_m / row_sums[:, np.newaxis]\n",
        "\n",
        "mask = np.ones(conf_m.shape, dtype=bool)\n",
        "np.fill_diagonal(mask, 0)\n",
        "max_value = conf_m[mask].max()\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "sns.heatmap(conf_m, annot=True, vmax=max_value, cmap=\"YlGnBu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpaeo-1IELLE"
      },
      "source": [
        "## Przykad 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN3XHhaxLkQ2"
      },
      "source": [
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ8_qPUkEupS"
      },
      "source": [
        "tensorboard_path = os.path.join(os.curdir, 'my_logs')\n",
        "def get_run_logdir():\n",
        "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
        "    return os.path.join(tensorboard_path, run_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt9m2ebmL1EQ"
      },
      "source": [
        "model = Sequential([\n",
        "    Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3))\n",
        "\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir())\n",
        "earlystop_cb = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "h = model.fit(X_train, y_train, epochs=50, validation_data=(X_valid, y_valid), \n",
        "          callbacks=[tensorboard_cb, earlystop_cb])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONEhxtMDL59s"
      },
      "source": [
        "mse_test = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNraDXCnE_d-"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./my_logs --port=6006"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_DEiuKM_BSb"
      },
      "source": [
        "## Przykad 3\n",
        "Analiza sentymentu - IMDB Reviews\n",
        "\n",
        "殴r贸do: https://huggingface.co/transformers/custom_datasets.html#seq-imdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNa-Olm2_AvK"
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piNTmZ-4CXSw"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZF3MWu0Iino"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqlib5u1Ano-"
      },
      "source": [
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "def read_imdb_split(split_dir, size):\n",
        "    split_dir = Path(split_dir)\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for label_dir in [\"pos\", \"neg\"]:\n",
        "        for text_file in (split_dir/label_dir).iterdir():\n",
        "            texts.append(text_file.read_text())\n",
        "            labels.append(0 if label_dir is \"neg\" else 1)\n",
        "\n",
        "    c = list(zip(texts, labels))\n",
        "    random.shuffle(c)\n",
        "    texts, labels = zip(*c)\n",
        "    return list(texts[:size]), list(labels[:size])\n",
        "\n",
        "\n",
        "train_texts, train_labels = read_imdb_split('aclImdb/train', size=1_000)\n",
        "test_texts, test_labels = read_imdb_split('aclImdb/test', size=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UFS2tdgSFSW"
      },
      "source": [
        "pd.DataFrame(list(zip(train_texts[:5], train_labels[:5])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDfQv1B8AqFl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYG5mbq_Ar-I"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN-mx5X4Ator"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI7fbEUXAvOz"
      },
      "source": [
        "import torch\n",
        "\n",
        "class IMDbDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
        "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
        "test_dataset = IMDbDataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_6HeKGuBXf7"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUQxBXqPF9IQ"
      },
      "source": [
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16, # batch size per device during training\n",
        "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    disable_tqdm=False,               # show some progress\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated  Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,             # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vaQ_TAOBGSb"
      },
      "source": [
        "trainer.evaluate(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdOP5lBiUcog"
      },
      "source": [
        "pred_labels = trainer.predict(test_dataset).label_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9dsB8ctPLiq"
      },
      "source": [
        "with pd.option_context('display.max_rows', 10, 'display.max_columns', None, 'display.max_colwidth', 150, 'display.expand_frame_repr', False):\n",
        "  print(pd.DataFrame(list(zip(test_texts, test_labels, pred_labels)), columns=['text', 'true', 'pred']).sample(n=20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab34d5m-5Vi8"
      },
      "source": [
        "## Kiedy u偶ywa sieci neuronowych:\n",
        "\n",
        "1. Kiedy mamy du偶y zbi贸r danych\n",
        "2. W zadaniach podobnych do tych, w kt贸rych kto ju偶 z powodzeniem u偶y sieci neuronowych\n",
        "3. Jeli istniej architektury dostosowane do naszego problemu\n",
        "4. Kiedy nie zale偶y nam na interpretowalnoci\n",
        "5. Gdy mamy dostpn du偶 moc obliczeniow do treningu\n",
        "6. Kiedy mamy du偶o czasu na przygotowanie modelu\n",
        "7. Gdy rozwizanie na produkcji nie musi dziaa bardzo szybko lub jestemy w stanie zapewni du偶 moc obliczeniowa r贸wnie偶 na produkcj\n",
        "8. Gdy zale偶y nam na (pewnej) odpornoci na szum na wejciu\n",
        "9. Jeli chcemy \"wycign\" reprezentacj z warstwy ukrytej\n",
        "10. Jeli zale偶y nam na pewnej elastycznoci wynikajcej z modularnej budowy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9IcyAddCTx4"
      },
      "source": [
        "## wiczenie \n",
        "Klasyfikacja obraz贸w - Cifar-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZObnfmoOS3hd"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "plt.imshow(X_train[0], cmap='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcHAnnkHS-fj"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=5_000)\n",
        "for s in [X_train, y_train, X_val, y_val, X_test, y_test]:\n",
        "    print(s.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}