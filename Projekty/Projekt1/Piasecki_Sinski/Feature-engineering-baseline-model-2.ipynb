{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "df_voice = pd.read_csv(\"./src/gender_voice_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voice.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie mamy żadnych brakujących wartości więc nie musimy się przejmować wypełnianiem braków."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_voice.columns[1:20]   \n",
    "for column in columns:\n",
    "    factor = 4\n",
    "    upper_lim = df_voice[column].mean () + df_voice[column].std () * factor\n",
    "    lower_lim = df_voice[column].mean () - df_voice[column].std () * factor\n",
    "    df_voice = df_voice[(df_voice[column] < upper_lim) & (df_voice[column] > lower_lim)]\n",
    "df_voice.reset_index(inplace = True)\n",
    "df_voice.drop(axis = 1, columns = ['index'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_voice.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby pozbyć się wartości odstających postanowiliśmy usunąć te wartości, które znajdują się w dowolnej kolumnie dalej od średniej niż 4 odchylenia standardowe. Straciliśmy w ten sposób około 200 obserwacji (~ 7 %)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usuwamy mocno skorelowane kolumny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = df_voice.corr().abs()\n",
    "s = c.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\")\n",
    "so = so.loc[(so < 1) & (so > 0.8)]\n",
    "so[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bardzo dużo kolumn jest skorelowanych, więc aby uprościć nasz model usuwamy kolumny, których współczynnik skorelowania jest wyższy niż 0.8 . W przypadku decyzji, którą z kolumn usunąć, usuwaliśmy tą która była w mniejszym stopniu skorelowana z targetem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voice.drop(axis = 1, columns=['maxdom','meanfreq','centroid','skew',\"sfm\",\"sp.ent\",\"sd\"],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_voice.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_voice.hist(bins = 40, figsize=(18, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standaryzacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standaryzujemy dane aby były zcentrowane do zera i żeby ich wariancje były tego samego rzędu. Robimy to po to, aby żadna zmienna nie dominowała i model mógł się uczyć na wszystkich zmiennych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(df_voice.drop(\"label\",axis=1))\n",
    "df_voice_standard = pd.DataFrame(scaler.transform(df_voice.drop(\"label\",axis=1)),columns=['median','Q25','Q75','IQR','kurt','mode','meanfun','minfun','maxfun','meandom','mindom','dfrange','modindx'])\n",
    "df_voice_standard.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voice_standard.hist(bins = 40, figsize=(18, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyskretyzacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dyskretyzacji używamy dla nieregularnych rozkładów *mode i dfrange* aby nasze modele lepiej sobie z nimi radziły."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disc = preprocessing.KBinsDiscretizer(n_bins=20, encode='ordinal',strategy=\"uniform\").fit(df_voice_standard[['mode','dfrange']])\n",
    "df_voice_standard[['mode','dfrange']] = disc.transform(df_voice_standard[['mode','dfrange']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_voice_standard.hist(bins = 40, figsize=(18, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszy model baselinowy to najprostsza instrukcja warunkowa, na kolumnie \"meanfun\", w której wartości bardzo ładnie separują się w zależności od targetu. Jak zobaczymy już tak banalny model daje nam trafność na poziomie 95 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voice_disc['label'] = df_voice[['label']]\n",
    "df_voice_standard['label'] = df_voice[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(data):\n",
    "    ## warunek meanfun < 0.13 -> male, otherwise: female\n",
    "    suma = 0\n",
    "    for i in range(len(data)):\n",
    "        if(data[\"meanfun\"][i] < 0.14):\n",
    "            predicted = 'male'\n",
    "        else:\n",
    "            predicted = 'female'\n",
    "        if(predicted == data['label'][i]):\n",
    "            suma = suma + 1\n",
    "    return suma / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model(df_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model x2\n",
    "Drugi baselinowym modelem będzie regresja logistyczna bez regularyzacji. Trafność modelu to 98 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ze standaryzacją \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "df_voice_standard['label'] = df_voice[['label']]\n",
    "X = df_voice_standard.drop('label',axis=1)\n",
    "y=df_voice_standard[['label']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "lr_base = LogisticRegression(max_iter=1000,penalty=\"none\").fit(X_train,y_train)\n",
    "y_pred = lr_base.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
