{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11 - Redukcja wymiarowości\n",
    "- PCA\n",
    "- t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T07:45:38.398163Z",
     "start_time": "2020-05-29T07:45:38.392870Z"
    }
   },
   "source": [
    "## PCA (Principal Component Analysis)\n",
    "Analiza głównych składników jest szybką metodą redukcji wymiarowości danych.\n",
    "Polega na znalezieniu liniowej transformacji zbioru zmiennych początkowych w mniej liczny zbiór zmiennych, zwanych składowymi głównymi. Jest to pewien sposób kompresji danych, dlatego metodę tę można wykorzystać w wielu dziedzinach nauki.\n",
    "\n",
    "\n",
    "**Algorytm PCA:**\n",
    "\n",
    "1. Standaryzacja danych\n",
    "\n",
    "2. Wyznaczenie macierzy kowariancji $Σ$ między zmiennymi początkowymi\n",
    "$$\n",
    "Σ=\\left(\\begin{array}{cc} \n",
    "var(x_1) & cov(x_1,x_2) & ... & cov(x_1,x_n) \\\\\n",
    "cov(x_1, x_2) & var(x_2) & ... & cov(x_2,x_n)\\\\\n",
    "... & ... &... & ...\\\\\n",
    "cov(x_1, x_2) & cov(x_2,x_n) & ... & var(x_n)\\\\\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "3. Wyznaczenie wartości własnych macierzy kowariancji poprzez rozwiązanie równania charakterystycznego $|Σ -λI|=0$\n",
    "\n",
    "4. Wybranie $k$ największych wartości własnych $λ$ i wyznaczenie dla nich wektorów własnych $Z_i$ zgodnie ze wzorem: $(Σ-λ_iI) * Z_i=0$\n",
    "\n",
    "5. Utworzenie macierzy przekształcenia liniowego $W$, bazującej na wektorach własnych\n",
    "\n",
    "6. Przekształcenie zmiennych początkowych według wzoru $Y = WX$\n",
    "\n",
    "![](./fig/pca_matrix.png)\n",
    "![](./fig/pca_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykład dla sztucznych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:06.105676Z",
     "start_time": "2020-05-30T11:24:04.912877Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:06.166903Z",
     "start_time": "2020-05-30T11:24:06.109775Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:06.645832Z",
     "start_time": "2020-05-30T11:24:06.169711Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jeden komponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:06.784690Z",
     "start_time": "2020-05-30T11:24:06.650136Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:06.793552Z",
     "start_time": "2020-05-30T11:24:06.786721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kierunki zmiennych PCA\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:06.801581Z",
     "start_time": "2020-05-30T11:24:06.797056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wyjaśniona wariancja - dla celów dydaktycznych (w praktyce raczej analizujemy procent wyjaśnionej wariancji)\n",
    "print(f'Total variance: {np.sqrt(np.std(X)):.4f}')\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:06.811970Z",
     "start_time": "2020-05-30T11:24:06.802893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Procent wyjaśnionej wariancji\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:07.028850Z",
     "start_time": "2020-05-30T11:24:06.814380Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=4,\n",
    "                    shrinkA=0, shrinkB=0, color='black')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal')\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 komponenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:07.038025Z",
     "start_time": "2020-05-30T11:24:07.031239Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:07.048591Z",
     "start_time": "2020-05-30T11:24:07.041093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kierunki zmiennych PCA\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:07.059277Z",
     "start_time": "2020-05-30T11:24:07.049859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wyjaśniona wariancja \n",
    "print(f'Total variance: {np.sqrt(np.std(X)):.4f}')\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:07.065004Z",
     "start_time": "2020-05-30T11:24:07.060677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Procent wyjaśnionej wariancji\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:07.268166Z",
     "start_time": "2020-05-30T11:24:07.065966Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal')\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dobieranie odpowiedniej liczby komponentów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major League Baseball Data from the 1986 and 1987 seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:07.288486Z",
     "start_time": "2020-05-30T11:24:07.269406Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hitters = pd.read_csv('hitters.csv')\n",
    "hitters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wydzielenie zmiennej celu i podział na zbiór treningowy i testowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:07.295754Z",
     "start_time": "2020-05-30T11:24:07.289629Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = hitters['NewLeague']\n",
    "X = hitters.drop(['Name','NewLeague','League','Division'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T11:24:07.530502Z",
     "start_time": "2020-05-30T11:24:07.296713Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA().fit(X_train)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zobaczmy jak poradzi sobie klasyfikacja dla surowych danych i dla PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Surowe dane\n",
    "y_hat = model.fit(X_train,y_train).predict(X_test)\n",
    "print(f'Accuracy bez PCA: {accuracy_score(y_test, y_hat):.4f}')\n",
    "\n",
    "# PCA - odczytane z wykresu\n",
    "comp_acc_pairs = []\n",
    "for i in range(1,17):\n",
    "    pca = PCA(n_components=i)\n",
    "    y_hat = model.fit(pca.fit_transform(X_train),y_train).predict(pca.transform(X_test))\n",
    "    comp_acc_pairs.append((i, accuracy_score(y_test, y_hat)))\n",
    "    \n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(*list(zip(*comp_acc_pairs)),'bx-')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytanie: Jak zwizualizować dane wielowymiarowe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-distributed Stochastic Neighbor Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cel: Pokazać ukryte zależności wielowymiarowych danych w 2D lub 3D.\n",
    "\n",
    "Idea: Chcemy, aby obserwacje podobne do siebie w wielu wymiarach były blisko siebie w podprzestrzeni.\n",
    "\n",
    "Wyróżnijmy:  \n",
    "**obserwacja x** - wielowymiarowy wektor cech o wymiarze zgodnym z wymiarem danych.  \n",
    "**mapowanie y** - dwu- lub trzy- wymiarowy wektor określający pozycje obserwacji na mapie.\n",
    "\n",
    "1. Obliczamy odległosci pomiędzy obserwacjami poprzez wyliczenie odpowiednich prawdopodobieństw warunkowych.\n",
    "Im bardziej obserwacje są podobne do siebie, tym większe prawdopodobieństwo. \n",
    "\n",
    "$$p_{j|i} = \\frac{\\exp{(-d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) / (2 \\sigma_i^2)})}{\\sum_{i \\neq k} \\exp{(-d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) / (2 \\sigma_i^2)})}, \\quad p_{i|i} = 0,$$\n",
    "\n",
    "Powyższy wzorek określa podobieństwo dwóch obserwacji z wykorzystaniem rozkładu normalnego scentrowanego w $x_{i}$. $\\sigma_{i}$ - odpowiednik odchylenia standardowego. Wyznaczanie $\\sigma_{i}$ - odsyłam do pracy źródłowej - \t\n",
    "Visualizing Data using t-SNE\n",
    "L. van der Maaten, and G. Hinton. Journal of Machine Learning Research (2008)\n",
    "\n",
    "Aby skorzystać z symetrii wprowadzamy:  \n",
    "\n",
    "$$p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}.$$\n",
    "\n",
    "W ten sposób otrzymujemy macierz podobieństwa **P** (ang. *similarity matrix*)\n",
    "\n",
    "2. Następnie rozważamy macierz **Q** stanowiącą podobne odwzorowanie co powyżej, ale dla mapowań. Z przyczyn, które na razie pomińmy prawdopodobieństwa nie są liczone z wykorzystaniem rozkładu normalnego, ale t-Studenta o jednym stopniu swobody (rozkład Cauchy'ego):\n",
    "\n",
    "$$q_{ij} = \\frac{(1 + ||\\boldsymbol{y}_i - \\boldsymbol{y}_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||\\boldsymbol{y}_k - \\boldsymbol{y}_l||^2)^{-1}},$$\n",
    "\n",
    "3. Zaszliśmy daleko. Rozważamy teraz takie \"umiejscowienie\" obserwacji w przestrzeni o 2 lub 3 wymiarach, aby jak najbardziej zminimalizować różnicę pomiędzy dwiema macierzami (P i Q). Zmieniamy oczywiście macierz mapowań **Q**. Odpowiada to minimalizacji dywergencji Kullbacka-Leibera (https://pl.wikipedia.org/wiki/Dywergencja_Kullbacka-Leiblera, https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):\n",
    "\n",
    "$$KL(P|Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/caltech101_tsne.jpg\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Użyjemy zbioru :https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Zawiera on 1797 obrazków cyfr w formacie 8x8 pikseli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Przykładowe obserwacje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 2, 5\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.gray()\n",
    "for i in range(ncols * nrows):\n",
    "    ax = plt.subplot(nrows, ncols, i + 1)\n",
    "    ax.matshow(digits.images[i,...])\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.title(digits.target[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Uporządkowanie danych (tylko, aby pomóc nam w wizualizacji, algorytm tego nie wymaga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([digits.data[digits.target==i]\n",
    "               for i in range(10)])\n",
    "y = np.hstack([digits.target[digits.target==i]\n",
    "               for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Wywołanie PCA i wizualizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(x, colors):\n",
    "    palette = np.array(sns.color_palette(\"hls\", 10))\n",
    "\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # add labels\n",
    "    txts = []\n",
    "    for i in range(10):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "digits_proj_pca = pca.fit_transform(X)\n",
    "scatter(digits_proj_pca, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Wywołanie t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random_state = 1500100900\n",
    "tSNE = TSNE(random_state=random_state, verbose=1)\n",
    "digits_proj = tSNE.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(digits_proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tSNE.fit(X).get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. t-SNE bywa kosztowne obliczeniowo nawet dla niewielkich zbiorów danych. Aby skrócić czas obliczeń, jeśli liczba cech jest znacząca, można połączyć oba podejścia: PCA + tSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca =  PCA().fit(X)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Spróbujmy z `n_components=40`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(PCA(n_components=40).fit(X).explained_variance_ratio_)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_pca =  PCA(n_components=40).fit_transform(X)\n",
    "random_state = 1500100900\n",
    "digits_proj = TSNE(random_state=random_state).fit_transform(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(digits_proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Dlaczego t?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W algorytmie SNE zarówno dla macierzy **P** (odwzorowuje odległości między obserwacjami) jak i **Q** (między mapowaniami) wykorzystuje się rozkład Gaussa. Okazuje się, że prowadzi to często do zagęszczenia mapowań. Obserwacje średnio odległe od siebie uzyskują bliskie sobie mapowania. (the crowding problem)\n",
    "\n",
    "Problem ten można zniwelować poprzez wykorzystanie dla mapowań rozkładu t-Studenta z jednym stopniem swobody (rozkład Cauchy'ego), który pozwala na lepsze odwzorowanie tych odległości dzięki własności grubych ogonów. Prowadzi to do lepszego odseparowania danych w mapowaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(0., 5., 1000)\n",
    "gauss = np.exp(-z**2)\n",
    "cauchy = 1/(1+z**2)\n",
    "plt.plot(z, gauss, label='Gaussian distribution')\n",
    "plt.plot(z, cauchy, label='Cauchy distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Uwagi końcowe o t-SNE:\n",
    " - służy do wizualizacji danych wielowymiarowych,\n",
    " - zwykle znajduje zastosowanie dla danych od 5 do 50 wymiarów, \n",
    " - wykorzystuje rozkład t-Studenta zamiast rozkładu normalnego (SNE), aby przeciwdziałać zbyt małym odstępom pomiędzy średnio-odległymi obserwacjami,\n",
    " - często jest wykorzystywane wraz z PCA, aby zmiejszyć czas obliczeń. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Źródła:\n",
    "   -  https://github.com/oreillymedia/t-SNE-tutorial,\n",
    "   -  http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf,\n",
    "   -  https://nbviewer.jupyter.org/urls/gist.githubusercontent.com/AlexanderFabisch/1a0c648de22eff4a2a3e/raw/59d5bc5ed8f8bfd9ff1f7faa749d1b095aa97d5a/t-SNE.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd01d5d51aab73d9610ec47b9b307b85bba458be1808c0087ceef2f39693ec1d43d",
   "display_name": "Python 3.9.2 64-bit ('vis': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}